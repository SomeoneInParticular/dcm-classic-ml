{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d199ddb1-292a-4a7c-b7a0-0b6174d3cfa0",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5052fd7-31a0-44eb-873c-fde1985b7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, permutations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlite3 import connect\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(707260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d8e32-1da6-4701-b5f3-b051f782483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = connect('../results.db')\n",
    "tables = pd.read_sql(\n",
    "    \"SELECT * FROM sqlite_master\", \n",
    "    con=con\n",
    ").loc[:, 'name']\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b1713-9069-4e57-9c12-aee466db015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = {}\n",
    "\n",
    "bad_vals = 0\n",
    "\n",
    "df_idx = ['dataset', 'model', 'weight', 'ori', 'prep']\n",
    "\n",
    "con = connect('../results.db')\n",
    "for t in tables:\n",
    "    # Pull the dataframe from the database\n",
    "    df = pd.read_sql(\n",
    "        f\"SELECT * FROM {t}\", \n",
    "        con=con\n",
    "    )\n",
    "\n",
    "    # If the table represents a study which wasn't run to completion, end early and report it\n",
    "    if df.shape[0] < 1000:\n",
    "        # print(f\"Study {t} was not completed\")\n",
    "        bad_vals += 1\n",
    "        continue\n",
    "\n",
    "    # Split the DataFrame's label into its components\n",
    "    label_comps = t.split('__')\n",
    "\n",
    "    # Pull the model label from it\n",
    "    model = label_comps[1]\n",
    "\n",
    "    # The rest of the components are in the final tag\n",
    "    final_comps = label_comps[-1].split('_')\n",
    "    if final_comps[0] == 'full':\n",
    "        dataset = \"full\"\n",
    "        ori = final_comps[1]\n",
    "        weight = final_comps[2]\n",
    "        prep = '_'.join(final_comps[3:])\n",
    "        df_key = \"_\".join([dataset, model, ori, weight, prep])\n",
    "    elif final_comps[0] == 'img':\n",
    "        dataset = 'img'\n",
    "        ori = final_comps[2]\n",
    "        weight = final_comps[3]\n",
    "        prep = '_'.join(final_comps[4:])\n",
    "        df_key = \"_\".join([dataset, model, ori, weight, prep])\n",
    "    elif final_comps[0] == 'clinical':\n",
    "        dataset = 'clinical'\n",
    "        ori = 'none'\n",
    "        weight = 'none'\n",
    "        prep = '_'.join(final_comps[2:])\n",
    "        df_key = \"_\".join([dataset, model, ori, weight, prep])\n",
    "    \n",
    "    # Store the components in the dataframe itself\n",
    "    df['dataset'] = dataset\n",
    "    df['model'] = model\n",
    "    df['weight'] = weight\n",
    "    df['ori'] = ori\n",
    "    df['prep'] = prep\n",
    "    # Track the resulting dataframe via the result\n",
    "    df_map[df_key] = df\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(f\"\\nTotal No. bad values: {bad_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcfadc6-03de-4f79-869e-4aea50a00e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871d17a-daa4-4cec-bd9a-f89f0cc8a997",
   "metadata": {},
   "source": [
    "# Best By Replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9f246-2897-4dac-84df-53dc16ef1f14",
   "metadata": {},
   "source": [
    "## Re-usable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff5e327-3558-4848-ab1e-49e55df8ec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_columns = [*df_idx, 'Mean', 'STD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae6837-02e2-40e2-aef0-45a63ce8e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute peak values by replicate, mean and std\n",
    "def get_peak1_of_value(target_value):\n",
    "    df_values = []\n",
    "    for k, df in df_map.items():\n",
    "        peak_df = df.sort_values(by=target_value).groupby('replicate').last()\n",
    "        peak_mean = np.mean(peak_df[target_value])\n",
    "        peak_std = np.std(peak_df[target_value], axis=0)\n",
    "        df_values.append(\n",
    "            [*df.loc[0, df_idx], peak_mean, peak_std]\n",
    "        )\n",
    "    peak_value_df = pd.DataFrame(data=df_values, index=df_map.keys(), columns=result_df_columns)\n",
    "    return peak_value_df.sort_values(by='Mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2baa3a8-a8a7-40fb-acbd-35390e026df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 5 values by replicate, mean and std\n",
    "def get_peak5_of_value(target_value):\n",
    "    df_values = []\n",
    "    for k, df in df_map.items():\n",
    "        peak_df = df.sort_values(by=target_value).groupby('replicate').tail(5)\n",
    "        peak_mean = np.mean(peak_df[target_value])\n",
    "        peak_std = np.std(peak_df[target_value], axis=0)\n",
    "        df_values.append(\n",
    "            [*df.loc[0, df_idx], peak_mean, peak_std]\n",
    "        )\n",
    "    peak_value_df = pd.DataFrame(data=df_values, index=df_map.keys(), columns=result_df_columns)\n",
    "    return peak_value_df.sort_values(by='Mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b549fb-63ec-4e23-9980-c6feb7b6cab1",
   "metadata": {},
   "source": [
    "## Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545a5e3e-4655-459e-b320-52e693b93785",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_peak1_of_value('balanced_accuracy (test)').set_index(df_idx).tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d766efe-373a-48f6-b14c-dbbc2c9a51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_peak5_of_value('balanced_accuracy (test)').set_index(df_idx).tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3d404-e520-4ee5-9734-2ee0e00ad774",
   "metadata": {},
   "source": [
    "# Performance Across Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9383a84-3b77-4352-9e45-6b47fbd3151b",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887485e-b0f3-4ff0-97bc-a82ba7ea9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_all_df_metrics(cols: list):\n",
    "    sub_dfs = []\n",
    "    for df in df_map.values():\n",
    "        sub_df = df.loc[:, [*df_idx, 'replicate', 'trial', *cols]]\n",
    "        sub_dfs.append(sub_df)\n",
    "    return pd.concat(sub_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff682b-347b-4004-8adb-8470e7b8b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_performance_across_trials(df, metric, grouping, fname):\n",
    "    # Plot the average and standard deviation\n",
    "    sns.lineplot(data=df, x='trial', y=metric, hue=grouping)\n",
    "\n",
    "    # Add details\n",
    "    plt.title(f'By {grouping} (Average)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    plt.savefig(f'figures/{fname}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272eba8e-7585-4c22-93be-3982113586b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_max_performance_across_trials(df, metric, grouping, fname):\n",
    "    # Reformat the data to be max by trial/replicate grouping\n",
    "    tmp_df = df.groupby(['replicate', 'trial', grouping])[metric].max().reset_index()\n",
    "    \n",
    "    # Plot the average and standard deviation\n",
    "    sns.lineplot(data=tmp_df, x='trial', y=metric, hue=grouping)\n",
    "\n",
    "    # Add details\n",
    "    plt.title(f'By {grouping} (Max)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    plt.savefig(f'figures/{fname}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b116a5-ab3e-4387-b1f3-626b8e91b0c7",
   "metadata": {},
   "source": [
    "## Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bedb85-c883-47c0-ac9c-b11658e667cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "bacc_avg_df = stack_all_df_metrics(['balanced_accuracy (test)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50853d17-873a-4d58-af61-8f3a1d4214a3",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4bd03-6f7c-4fa5-8fea-b2ebe4ba9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_performance_across_trials(bacc_avg_df, 'balanced_accuracy (test)', 'dataset', 'bacc_avg_by_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45c681-6846-43c7-9c88-ee70a84ec81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_performance_across_trials(bacc_avg_df, 'balanced_accuracy (test)', 'dataset', 'bacc_max_by_dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208e3214-3968-4b52-a4d4-869e18c7f6a4",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fdf94-fbeb-4729-9716-aa6915bc3ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_performance_across_trials(bacc_avg_df, 'balanced_accuracy (test)', 'model', 'bacc_avg_by_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae0ec02-709f-4b1b-b473-14de983cef8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_performance_across_trials(bacc_avg_df, 'balanced_accuracy (test)', 'model', 'bacc_max_by_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb85b9d8-8e9d-43a3-b81c-1ccd765881eb",
   "metadata": {},
   "source": [
    "### Image Contrast (Weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed729c63-a67d-4cc0-bd8f-73c70759dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_performance_across_trials(bacc_avg_df, 'balanced_accuracy (test)', 'weight', 'bacc_avg_by_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67c717-0939-45cc-8057-c1b4375b55a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_performance_across_trials(bacc_avg_df, 'balanced_accuracy (test)', 'weight', 'bacc_max_by_weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e2885-a20a-43fd-bedb-7755143140f1",
   "metadata": {},
   "source": [
    "### Image Orientation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7b8845-4e9d-4dce-afad-0266a27f92f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_performance_across_trials(bacc_avg_df, 'balanced_accuracy (test)', 'ori', 'bacc_avg_by_ori')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bbb2d4-c361-4933-b33c-347b5e16153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_performance_across_trials(bacc_avg_df, 'balanced_accuracy (test)', 'ori', 'bacc_max_by_ori')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbae93c7-e948-4947-be69-020f27bdd0a1",
   "metadata": {},
   "source": [
    "### Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72176a31-b966-4366-84a7-128f5b724cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_performance_across_trials(bacc_avg_df, 'balanced_accuracy (test)', 'prep', 'bacc_avg_by_prep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbbd404-5940-4007-a9c7-e07103389ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_max_performance_across_trials(bacc_avg_df, 'balanced_accuracy (test)', 'prep', 'bacc_max_by_prep')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e99e8b-13ee-4ca6-8486-ac69f7d9ba8c",
   "metadata": {},
   "source": [
    "# Paired T-Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb963b73-3a75-4c9c-a6ca-246f42f857cf",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb0928-c546-4b1f-bc10-47c76357ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, permutations\n",
    "\n",
    "from scipy.stats import normaltest, ranksums"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74469ad1-a268-4bb7-9781-9662e5752be9",
   "metadata": {},
   "source": [
    "Target metric gathering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e29e5-bfaa-406d-9228-4f01903673e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute peak values by replicate, mean and std\n",
    "def get_best_per_replicate(target_value):\n",
    "    component_dfs = []\n",
    "    for k, df in df_map.items():\n",
    "        peak_df = df.sort_values(by=target_value).groupby('replicate').last()\n",
    "        peak_df = peak_df.loc[:, [*df_idx, 'trial', target_value]]\n",
    "        component_dfs.append(peak_df)\n",
    "    result_df = pd.concat(component_dfs).reset_index()\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd440e39-1f2f-4b15-8010-623c78be99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_normality(df, query_key, target):\n",
    "    isnormal = {}\n",
    "    query_set = set(replicate_best_bacc_df[query_key])\n",
    "\n",
    "    for k in query_set:\n",
    "        x = df.query(f\"{query_key} == '{k}'\")[target]\n",
    "        isnormal[k] = [normaltest(x).pvalue]\n",
    "\n",
    "    # Save the results as a dataframe\n",
    "    return_df = pd.DataFrame.from_dict(isnormal).T\n",
    "    return_df.columns = ['p-value']\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c576c18-a23f-4aac-a534-1fccac6d7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_keys = {\n",
    "    'two-sided': '!=',\n",
    "    'greater':   '>',\n",
    "    'less':      '<'\n",
    "}\n",
    "\n",
    "def paired_rankedsum(df, query, target, alternative='two-sided'):\n",
    "    pvals = {}\n",
    "    query_set = set(df[query])\n",
    "\n",
    "    # Caclulate the native rankedsum p-value for each pair of datasets, testing whether the former's value is greater than the latters\n",
    "    for v1, v2 in permutations(query_set, 2):\n",
    "        x1 = df.query(f\"{query} == '{v1}'\")[target]\n",
    "        x2 = df.query(f\"{query} == '{v2}'\")[target]\n",
    "        p = ranksums(x1, x2, alternative=alternative).pvalue\n",
    "        pvals[f\"{v1} {alt_keys[alternative]} {v2}\"] = [p]\n",
    "\n",
    "    # Save the results as a dataframe\n",
    "    return_df = pd.DataFrame.from_dict(pvals).T\n",
    "    return_df.index.name = 'Comparison'\n",
    "    return_df.columns = ['p']\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a754f-899f-45c9-a9ea-d1a1e21b2fc0",
   "metadata": {},
   "source": [
    "## Testing Balanced Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e481c6b-4bba-422c-b1e0-0c487b8b2fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'balanced_accuracy (test)'\n",
    "replicate_best_bacc_df = get_best_per_replicate(target)\n",
    "replicate_best_bacc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2d9a6-65fa-42cb-a326-8f2875e9c5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p-values for whether one experimental permutation has greater average balanced accuracy performance than another\n",
    "sub_dfs = []\n",
    "for k in df_idx:\n",
    "    tmp_df = paired_rankedsum(replicate_best_bacc_df, k, target, alternative='greater')\n",
    "    sub_dfs.append(tmp_df)\n",
    "\n",
    "sig_df = pd.concat(sub_dfs).sort_values('p')\n",
    "\n",
    "# Calculate the corrected p-value significance as well\n",
    "n_samples = sig_df.shape[0]\n",
    "sig_df['significance'] = ''\n",
    "for i, t in enumerate([0.05, 0.01, 0.001]):\n",
    "    sig_df.loc[sig_df['p']*n_samples < t, 'significance'] = '*'*(i+1)\n",
    "\n",
    "sig_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f8e27-6b04-40cd-8153-3d12f85307c5",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74bd963-7fc0-49ae-b481-c667a37a7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_feature_imp(val):\n",
    "    # Strip leading and trailing brackets\n",
    "    val = val[1:-2]\n",
    "\n",
    "    # Create a dictionary from the remaining components\n",
    "    imp_dict = dict()\n",
    "    for v in val.split(', '):\n",
    "        vcomps = v.split(': ')\n",
    "        k = ': '.join(vcomps[:-1])\n",
    "        v = float(vcomps[-1])\n",
    "        imp_dict[k] = v\n",
    "        \n",
    "    return imp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6d00a-69b7-44e1-870b-41de43d7ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_report(df: pd.DataFrame, weight_col, feature_col):\n",
    "    # Convert the dictionaries contained with the feature_col dicts into dataframes which can be stacked\n",
    "    raw_dfs = []\n",
    "    weighted_dfs = []\n",
    "    for r in df.iterrows():\n",
    "        rvals = r[1]\n",
    "        tmp_df = pd.DataFrame.from_dict({k: [v] for k, v in rvals[feature_col].items()})\n",
    "        weight = rvals[weight_col]\n",
    "        raw_dfs.append(tmp_df)\n",
    "        weighted_dfs.append(tmp_df * weight)\n",
    "    raw_feature_imps = pd.concat(raw_dfs).fillna(0)\n",
    "    weighted_feature_imps = pd.concat(weighted_dfs).fillna(0)\n",
    "\n",
    "    # Interpret the results into a clean report\n",
    "    feature_imp_report = {\n",
    "        \"Mean (Raw)\": raw_feature_imps.mean(),\n",
    "        \"STD (Raw)\": raw_feature_imps.std(),\n",
    "        \"Mean (Magnitude)\": np.abs(raw_feature_imps).mean(),\n",
    "        \"STD (Magnitude)\": np.abs(raw_feature_imps).std(),\n",
    "        \"Mean (Performance Weighted)\": weighted_feature_imps.mean(),\n",
    "        \"STD (Performance Weighted)\": weighted_feature_imps.std(),\n",
    "    }\n",
    "    result_df = pd.DataFrame.from_dict(feature_imp_report)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1798fd-75d9-4364-8c9a-c549a655b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate and stack the information relative to the value\n",
    "sub_dfs = []\n",
    "\n",
    "for df in df_map.values():\n",
    "    tmp_df = df.loc[:, ['replicate', 'trial', *df_idx, 'balanced_accuracy (test)', 'importance_by_permutation (test)']]\n",
    "    sub_dfs.append(tmp_df)\n",
    "\n",
    "feature_imp_df = pd.concat(sub_dfs)\n",
    "\n",
    "# Isolate only the best trial from each replicate\n",
    "feature_imp_df = feature_imp_df.sort_values('balanced_accuracy (test)').groupby([*df_idx, 'replicate']).tail(1).set_index(df_idx)\n",
    "\n",
    "# Parse the feature importance list into a cleaner dictionary\n",
    "feature_imp_df['importance_by_permutation (test)'] = feature_imp_df['importance_by_permutation (test)'].apply(format_feature_imp)\n",
    "feature_imp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebe870-c234-4d7d-9b4e-a42363110f70",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2f803-a8ac-4cb2-9c2e-8db2442e39e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_pca(val): return 'pca' in val \n",
    "\n",
    "pca_feature_imp_df = feature_imp_df.reset_index().loc[feature_imp_df.reset_index()['prep'].apply(is_pca), :].set_index([*df_idx])\n",
    "pca_feature_imp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fb1876-94bc-4e42-860d-743a4577a627",
   "metadata": {},
   "source": [
    "### Raw Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2472b-9c5a-4984-ba62-c7b04eab4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpca_feature_imp_df = feature_imp_df.drop(pca_feature_imp_df.index)\n",
    "nonpca_feature_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2389dbae-f46b-4c4c-96c8-1638583e4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feature_imp_df = nonpca_feature_imp_df.query(\"dataset == 'full'\")\n",
    "full_feature_report = feature_importance_report(full_feature_imp_df, 'balanced_accuracy (test)', 'importance_by_permutation (test)')\n",
    "full_feature_report.sort_values(\"Mean (Performance Weighted)\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954bda7-4083-4d28-97c3-9381d279ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature_imp_df = nonpca_feature_imp_df.query(\"dataset == 'img'\")\n",
    "img_feature_report = feature_importance_report(img_feature_imp_df, 'balanced_accuracy (test)', 'importance_by_permutation (test)')\n",
    "img_feature_report.sort_values(\"Mean (Performance Weighted)\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c18647-7cb0-40c0-82a8-a277a72110a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_feature_imp_df = nonpca_feature_imp_df.query(\"dataset == 'clinical'\")\n",
    "clin_feature_report = feature_importance_report(clin_feature_imp_df, 'balanced_accuracy (test)', 'importance_by_permutation (test)')\n",
    "clin_feature_report.sort_values(\"Mean (Performance Weighted)\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e1783-2043-4fc9-a391-80d698760000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
