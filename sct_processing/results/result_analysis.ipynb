{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf01c257-225d-4646-9315-3129cf3a99a0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d199ddb1-292a-4a7c-b7a0-0b6174d3cfa0",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5052fd7-31a0-44eb-873c-fde1985b7f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, permutations\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlite3 import connect\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "np.random.seed(707260)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d8e32-1da6-4701-b5f3-b051f782483e",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = connect('../results.db')\n",
    "tables = pd.read_sql(\n",
    "    \"SELECT * FROM sqlite_master\", \n",
    "    con=con\n",
    ").loc[:, 'name']\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814b1713-9069-4e57-9c12-aee466db015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_map = {}\n",
    "\n",
    "bad_vals = 0\n",
    "\n",
    "analysis_idx = ['seg_algo', 'dataset', 'model', 'weight', 'ori', 'prep']\n",
    "\n",
    "con = connect('../results.db')\n",
    "for t in tables:\n",
    "    # Pull the dataframe from the database\n",
    "    try:\n",
    "        df = pd.read_sql(\n",
    "            f\"SELECT * FROM {t}\", \n",
    "            con=con\n",
    "        )\n",
    "    except:\n",
    "        print(f\"Failed to read table {t}, ignoring it\")\n",
    "        bad_vals += 1\n",
    "        continue\n",
    "\n",
    "    # If the table represents a study which wasn't run to completion, end early and report it\n",
    "    if df.shape[0] < 1000:\n",
    "        # print(f\"Study {t} was not completed\")\n",
    "        bad_vals += 1\n",
    "        continue\n",
    "\n",
    "    # Split the DataFrame's label into its components\n",
    "    label_comps = t.split('__')\n",
    "\n",
    "    # Pull the model label from it\n",
    "    model = label_comps[1]\n",
    "\n",
    "    # The rest of the components are in the final tag\n",
    "    final_comps = label_comps[-1].split('_')\n",
    "\n",
    "    # Clinical needs to be treated special:\n",
    "    if final_comps[0] == 'clinical':\n",
    "        seg_algo = 'none'\n",
    "        dataset = 'clinical'\n",
    "        ori = 'none'\n",
    "        weight = 'none'\n",
    "        prep = '_'.join(final_comps[2:])\n",
    "    # The rest have the first index as the segmentation algorithm\n",
    "    else:\n",
    "        seg_algo = final_comps[0]\n",
    "        if final_comps[1] == 'full':\n",
    "            dataset = final_comps[1]\n",
    "            ori = final_comps[2]\n",
    "            weight = final_comps[3]\n",
    "            prep = '_'.join(final_comps[4:])\n",
    "        elif final_comps[1] == 'img':\n",
    "            dataset = '_'.join(final_comps[1:3])\n",
    "            ori = final_comps[3]\n",
    "            weight = final_comps[4]\n",
    "            prep = '_'.join(final_comps[5:])\n",
    "        \n",
    "    df_key = \"_\".join([seg_algo, dataset, model, ori, weight, prep])\n",
    "    \n",
    "    # Store the components in the dataframe itself\n",
    "    df['seg_algo'] = seg_algo\n",
    "    df['dataset'] = dataset\n",
    "    df['model'] = model\n",
    "    df['weight'] = weight\n",
    "    df['ori'] = ori\n",
    "    df['prep'] = prep\n",
    "    \n",
    "    # Track the resulting dataframe via the result\n",
    "    df_map[df_key] = df\n",
    "\n",
    "con.close()\n",
    "\n",
    "print(f\"\\nTotal No. bad values: {bad_vals}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcfadc6-03de-4f79-869e-4aea50a00e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e58198a-b430-42b3-91f0-09d5bf24afab",
   "metadata": {},
   "source": [
    "## Performance Metric Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5e4735-83b7-4310-b2c1-bd2f873148f0",
   "metadata": {},
   "source": [
    "All metrics in the below index list are tracked for all analyses, so are safe to query (and stack) from all analytical permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c75c94-4887-42b6-9c5b-605f68790511",
   "metadata": {},
   "outputs": [],
   "source": [
    "shared_performance_metric_idxs = [\n",
    "    \"objective\",\n",
    "    \"balanced_accuracy (validate)\",\n",
    "    \"roc_auc (validate)\",\n",
    "    \"log_loss (validate)\",\n",
    "    \"balanced_accuracy (test)\",\n",
    "    \"roc_auc (test)\",\n",
    "    \"log_loss (test)\",\n",
    "    \"importance_by_permutation (test)\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091a76c4-d55c-46c7-b7d3-10a6bc7f61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_idxs = [\n",
    "    \"replicate\",\n",
    "    \"trial\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f4aec0-32b5-4c31-b5b7-c75875ac3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_performance_metrics():\n",
    "    sub_dfs = []\n",
    "    for df in df_map.values():\n",
    "        sub_df = df.loc[:, [*analysis_idx, *study_idxs, *shared_performance_metric_idxs]]\n",
    "        sub_dfs.append(sub_df)\n",
    "    return pd.concat(sub_dfs)\n",
    "\n",
    "performance_metric_df = stack_performance_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f91753-4c84-4643-98b9-a0d63ea3cb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec27bd8-67fb-43dd-8a32-b87bf113a6ed",
   "metadata": {},
   "source": [
    "# Patient Metric Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61272b01-a632-4658-88b8-7fb962edef71",
   "metadata": {},
   "source": [
    "## Data Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcbb261-4dc6-47da-9d03-e48debb2f785",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinical_metric_df = pd.read_csv(\"../deepseg_data/clinical_only.tsv\", sep='\\t')\n",
    "clinical_metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f17c7e-c649-45d0-9900-3f446b28bca7",
   "metadata": {},
   "source": [
    "## mJOA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0329de6-5f8c-41ee-93bd-ae8f725977d9",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f052f31f-72a7-4136-9aff-81894efc81f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distributions(data, cmap, legend_elements, xlabel, title, mean_offset=0, flip_mean_rot=False):\n",
    "    # Get the appropriate ranges for the data\n",
    "    min_range = int(np.min(data))-1\n",
    "    max_range = int(np.max(data))+1\n",
    "    \n",
    "    # Bin the data\n",
    "    hist, bins = np.histogram(\n",
    "        data, \n",
    "        np.array(range(min_range, max_range))+.1\n",
    "    )\n",
    "    \n",
    "    # Generate the figure\n",
    "    fig, ax = plt.subplots()\n",
    "        \n",
    "    # Iteratively color code the bars\n",
    "    for t, c in cmap.items():\n",
    "        mask = bins < t\n",
    "        to_display = np.array(range(min_range, t))+0.5\n",
    "        vals = hist[mask[:-1]]\n",
    "        ax.bar(\n",
    "            to_display, vals,\n",
    "            width=1, color=c,\n",
    "            align='edge',\n",
    "            edgecolor='black'\n",
    "        )\n",
    "        \n",
    "    # Add a mean line\n",
    "    data_mean = np.mean(data)\n",
    "    ax.axvline(data_mean, ls='--', c='black')\n",
    "    if flip_mean_rot:\n",
    "        ax.text(data_mean-0.5, ax.get_ylim()[1]-mean_offset, f\"Mean ({data_mean:.4})\", rotation=90)\n",
    "    else:\n",
    "        ax.text(data_mean+0.05, ax.get_ylim()[1]-mean_offset, f\"Mean ({data_mean:.4})\", rotation=-90)\n",
    "        \n",
    "    # Add in the legend\n",
    "    ax.legend(handles=legend_elements)\n",
    "    \n",
    "    # Add in labels\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Return the figure and axis\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c418cf0c-0560-4a09-8a4f-ae8d7a8402e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limits so that all plots have consistent range\n",
    "xlim_min = int(np.min([*clinical_metric_df['mJOA initial'], *clinical_metric_df['mJOA 12 months']]))-1\n",
    "xlim_max = int(np.max([*clinical_metric_df['mJOA initial'], *clinical_metric_df['mJOA 12 months']]))+1\n",
    "\n",
    "ylim_min = 0\n",
    "ylim_max = int(np.max([\n",
    "    *np.histogram(clinical_metric_df['mJOA initial'], np.array(range(xlim_min, xlim_max))+.1)[0],\n",
    "    *np.histogram(clinical_metric_df['mJOA 12 months'], np.array(range(xlim_min, xlim_max))+.1)[0]\n",
    "]))+5\n",
    "\n",
    "# Color threshold map\n",
    "severity_cmap = {\n",
    "    18: 'blue',\n",
    "    17: 'green',\n",
    "    14: 'gold',\n",
    "    11: 'red'\n",
    "}\n",
    "\n",
    "# Generate a custom legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='red', edgecolor='black', label='Severe'),\n",
    "    Patch(facecolor='gold', edgecolor='black', label='Moderate'),\n",
    "    Patch(facecolor='green', edgecolor='black', label='Mild'),\n",
    "    Patch(facecolor='blue', edgecolor='black', label='Healthy'),\n",
    "]\n",
    "\n",
    "# DCM Severity labelling\n",
    "clinical_metric_df['DCM Severity initial'] = 'Severe'\n",
    "clinical_metric_df.loc[clinical_metric_df['mJOA initial'] > 11, 'DCM Severity initial'] = 'Moderate'\n",
    "clinical_metric_df.loc[clinical_metric_df['mJOA initial'] > 14, 'DCM Severity initial'] = 'Mild'\n",
    "clinical_metric_df.loc[clinical_metric_df['mJOA initial'] > 17, 'DCM Severity initial'] = 'Healthy'\n",
    "\n",
    "clinical_metric_df['DCM Severity 12 months'] = 'Severe'\n",
    "clinical_metric_df.loc[clinical_metric_df['mJOA 12 months'] > 11, 'DCM Severity 12 months'] = 'Moderate'\n",
    "clinical_metric_df.loc[clinical_metric_df['mJOA 12 months'] > 14, 'DCM Severity 12 months'] = 'Mild'\n",
    "clinical_metric_df.loc[clinical_metric_df['mJOA 12 months'] > 17, 'DCM Severity 12 months'] = 'Healthy'\n",
    "\n",
    "# Output path for the files\n",
    "mjoa_dist_out_path = Path('figures/mjoa_dist')\n",
    "if not mjoa_dist_out_path.exists():\n",
    "    mjoa_dist_out_path.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c24b1a-ffce-4a81-85e2-8f788547c1e1",
   "metadata": {},
   "source": [
    "### Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab91fb5-9a81-4edf-bc08-997dfc2ac38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig, ax = plot_distributions(\n",
    "    clinical_metric_df['mJOA initial'], severity_cmap, legend_elements,\n",
    "    'mJOA', 'Pre-Treatment mJOA Scores (Full)', 20\n",
    ")\n",
    "\n",
    "# Plot the total number of each severity class as text\n",
    "severity_counts = clinical_metric_df['DCM Severity initial'].value_counts()\n",
    "ax.text(9, 15, f\"({severity_counts['Severe']})\", c='black', size=12, horizontalalignment='center')\n",
    "ax.text(14, 44.5, f\"({severity_counts['Moderate']})\", c='black', size=12, horizontalalignment='center')\n",
    "ax.text(16.5, 33, f\"({severity_counts['Mild']})\", c='black', size=12, horizontalalignment='center')\n",
    "ax.text(18, 2.5, f\"({severity_counts['Healthy']})\", c='black', size=12, horizontalalignment='center')\n",
    "\n",
    "# Save and show the result\n",
    "fig.savefig(mjoa_dist_out_path / 'pre_treatment_mjoa.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d113761-8c0c-4e35-af5c-ee50440fbfc7",
   "metadata": {},
   "source": [
    "### 12 Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d26d7cc-1fef-4474-a8e9-e4b24fb7785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig, ax = plot_distributions(\n",
    "    clinical_metric_df['mJOA 12 months'], severity_cmap, legend_elements,\n",
    "    'mJOA', 'Post-Treatment mJOA Scores (Full)', 20, flip_mean_rot=True\n",
    ")\n",
    "\n",
    "# Plot the total number of each severity class as text\n",
    "severity_counts = clinical_metric_df['DCM Severity 12 months'].value_counts()\n",
    "ax.text(8.5, 3, f\"({severity_counts['Severe']})\", c='black', size=12, horizontalalignment='center')\n",
    "ax.text(13.5, 36, f\"({severity_counts['Moderate']})\", c='black', size=12, horizontalalignment='center')\n",
    "ax.text(16, 45, f\"({severity_counts['Mild']})\", c='black', size=12, horizontalalignment='center')\n",
    "ax.text(18, 37, f\"({severity_counts['Healthy']})\", c='black', size=12, horizontalalignment='center')\n",
    "\n",
    "# Save and show the result\n",
    "fig.savefig(mjoa_dist_out_path / 'post_treatment_mjoa.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad01ffb3-73de-4f92-9529-f488ff396cf8",
   "metadata": {},
   "source": [
    "### mJOA Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28350353-d34b-4dd5-a596-d1885dd2a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new color scheme and legend for this new style of data\n",
    "delta_cmap = {\n",
    "    8: 'springgreen',\n",
    "    0: 'white',\n",
    "    -1: 'salmon'\n",
    "}\n",
    "\n",
    "delta_legend_elements = [\n",
    "    Patch(facecolor='springgreen', edgecolor='black', label='Improved'),\n",
    "    Patch(facecolor='white', edgecolor='black', label='No Change'),\n",
    "    Patch(facecolor='salmon', edgecolor='black', label='Declined'),\n",
    "]\n",
    "\n",
    "xticks = (\n",
    "    list(range(-8, 9, 2)),\n",
    "    list(range(-8, 9, 2))\n",
    ")\n",
    "\n",
    "deltas = clinical_metric_df['mJOA 12 months'] - clinical_metric_df['mJOA initial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea28752-a836-48ab-83b1-3567c4c23e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the deltas\n",
    "fig, ax = plot_distributions(\n",
    "    deltas, delta_cmap, delta_legend_elements, \n",
    "    \"mJOA Change\", 'Change in mJOA Over 1 Year (Full)', 20, flip_mean_rot=True\n",
    ")\n",
    "\n",
    "# Plot the total number of each severity class as text\n",
    "change_counts = pd.cut(\n",
    "    deltas, \n",
    "    [-20, -1, 0, 20], \n",
    "    labels=['Declined', 'No Change', 'Improved']\n",
    ").value_counts()\n",
    "ax.text(-4.5, 9, f\"({change_counts['Declined']})\", c='black', size=12, verticalalignment='center')\n",
    "ax.text(-0.6, 40, f\"({change_counts['No Change']})\", c='black', size=12, verticalalignment='center')\n",
    "ax.text(4, 32, f\"({change_counts['Improved']})\", c='black', size=12, verticalalignment='center')\n",
    "\n",
    "# Save and show the result\n",
    "fig.savefig(mjoa_dist_out_path / 'treatment_mjoa_delta.svg')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b446d5-eb86-43f6-a875-1b68faf8e95b",
   "metadata": {},
   "source": [
    "## Hirayabashi Recovery Ratio Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f5d7f7-e9b0-4b60-90bf-22c28b223f1e",
   "metadata": {},
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45fa902-ec49-44d3-80ab-5fd21a731fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Plot the KDE distribution onto an existing plot\n",
    "def plot_kde(ax, values, c='black', ls='-', label=None):\n",
    "    kde = gaussian_kde(values)\n",
    "    kde.covariance_factor = lambda: 0.15\n",
    "    kde._compute_covariance()\n",
    "    xs = np.linspace(np.min(values), np.max(values), 200)\n",
    "    ys = kde(xs)\n",
    "    ys /= np.linalg.norm(ys)\n",
    "    if label == None:\n",
    "        ax.plot(xs, ys, ls=ls, c=c)\n",
    "    else:\n",
    "        ax.plot(xs, ys, ls=ls, c=c, label=label)\n",
    "\n",
    "# Clean out invalid values from the set\n",
    "def clean_vals(df):\n",
    "    df2 = df[df != -np.inf]\n",
    "    df2 = df2.dropna()\n",
    "    return df2\n",
    "\n",
    "# Adds important reference lines to the plot\n",
    "def draw_line_references(ax):\n",
    "    # Significant improvement\n",
    "    ax.axvline(0.5, ls='-.', c='grey')\n",
    "    \n",
    "    # Baselines\n",
    "    ax.axhline(0, ls=\":\",  c='lightgrey') \n",
    "    ax.axvline(0, ls=\":\",  c='lightgrey')\n",
    "\n",
    "# The HRR Equation, for immediate reference within the plot\n",
    "hirabayashi_equation = r\"HRR = $\\frac{\\mathrm{mJOA (1 Year)} - \\mathrm{mJOA (Initial)}}{18 - \\mathrm{mJOA (Initial)}}$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb740816-3b59-45d3-9165-23b4d5044e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the HRR for our patients, skipping over initially healthy patients who could not improve whatsoever\n",
    "hrr_df = clinical_metric_df.loc[clinical_metric_df['DCM Severity initial'] != \"Healthy\", 'HRR']\n",
    "\n",
    "# Generate the initial plot\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot our reference lines\n",
    "draw_line_references(ax)\n",
    "\n",
    "# Plot the distributions by their initial severity class\n",
    "plot_kde(\n",
    "    ax, clean_vals(hrr_df[clinical_metric_df['DCM Severity initial'] == 'Severe']), ls='--', c='red', label='Severe'\n",
    ")\n",
    "plot_kde(\n",
    "    ax, clean_vals(hrr_df[clinical_metric_df['DCM Severity initial'] == 'Moderate']), ls='--', c='gold', label='Moderate'\n",
    ")\n",
    "plot_kde(\n",
    "    ax, clean_vals(hrr_df[clinical_metric_df['DCM Severity initial'] == 'Mild']), ls='--', c='green', label='Mild'\n",
    ")\n",
    "\n",
    "# Plot the overall distribution\n",
    "plot_kde(ax, hrr_df, c='blue', label='All')\n",
    "\n",
    "# Calculate the ratio above and below the HRR significance threshold, and add it\n",
    "good_ratio = np.sum(hrr_df >= 0.5)/hrr_df.shape[0]\n",
    "fair_ratio = np.sum(hrr_df < 0.5)/hrr_df.shape[0]\n",
    "\n",
    "ax.text(0.7, 0.238, f\"{good_ratio: .2f}\", c='purple')\n",
    "ax.text(-0.5, 0.238, f\"{fair_ratio: .2f}\", c='purple')\n",
    "\n",
    "# Add axis labels\n",
    "ax.set_xlabel('Hirabayashi Recovery Ratio (HRR)')\n",
    "ax.set_ylabel('Normalized Kernel Density Estimate')\n",
    "\n",
    "# Add a legend\n",
    "ax.legend(title='Pre-Surgical DCM Severity')\n",
    "\n",
    "# Add hirabayashi equation directly to plot\n",
    "ax.text(-8, 0.15, hirabayashi_equation)\n",
    "\n",
    "# Add a title\n",
    "ax.set_title(\"Distribution of Hirabayashi Recovery Ratio\")\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(mjoa_dist_out_path / 'hirabayashi_ratios.svg')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfd6a81-39c2-4261-80e7-27aeb53bfe14",
   "metadata": {},
   "source": [
    "## Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6b9b24-3298-41a2-95aa-b34716f64fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_continuous_demographics(col):\n",
    "    sns.displot(clinical_metric_df, x=col)\n",
    "    plt.title(f\"Patient Distribution ({col})\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/demo_dist/{'_'.join(col.lower().split(' '))}_dist.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c029e164-7998-49bb-87e3-413b738bdcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_demographic_cols = [\n",
    "    \"Age\",\n",
    "    \"BMI\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a357f53-8841-47cc-98d2-316f754b4bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in continuous_demographic_cols:\n",
    "    plot_continuous_demographics(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62aa0f7-224b-4472-b0c1-817ac77cce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_categorical_demographics(col):\n",
    "    col_counts = clinical_metric_df[col].value_counts()\n",
    "    plt.pie(col_counts, labels=None, autopct=lambda x: f'{x: .2f}%')\n",
    "    plt.legend(labels=col_counts.index)\n",
    "    plt.title(f\"Patient Distribution ({col})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/demo_dist/{'_'.join(col.lower().split(' '))}_dist.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb84c52-6ea9-4994-ab07-3cef87cfe408",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_demographic_cols = [\n",
    "    \"Sex\",\n",
    "    \"Work Status (Category)\",\n",
    "    \"Symptom Duration\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73311c11-b1cb-4810-8717-1df7d9e7c75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in categorical_demographic_cols:\n",
    "    plot_categorical_demographics(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3871d17a-daa4-4cec-bd9a-f89f0cc8a997",
   "metadata": {},
   "source": [
    "# Best across Trial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfdffd3-3046-46dc-bec6-197ed66bd4a3",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bb027-815a-4a52-917f-73a0e33aa27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_across_trials_idx = [*analysis_idx, 'Mean', 'STD']\n",
    "best_across_trials_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15301ad6-9a43-4ab8-8a46-761212991ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the values of one column when the value of another is among the n-highest (default to n=1)\n",
    "def get_peak_at_max_other(target_col, other_col, df=performance_metric_df, n=1) -> pd.DataFrame:\n",
    "    # Get the best value per analytical grouping and replicate across all trials\n",
    "    peak_value_df = df.sort_values(by=other_col).groupby([*analysis_idx, 'replicate']).tail(n)\n",
    "\n",
    "    # Set up the return dataframe\n",
    "    analysis_groups = peak_value_df.reset_index().groupby(analysis_idx)\n",
    "    value_means = analysis_groups[target_col].mean()\n",
    "    value_stds = analysis_groups[target_col].std()\n",
    "    return_df = pd.DataFrame(index=list(value_means.index))\n",
    "    return_df['Mean'] = value_means\n",
    "    return_df['STD'] = value_stds\n",
    "\n",
    "    # Return the result\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133d2a1c-0b11-4b33-b9c6-67dfc5264195",
   "metadata": {},
   "source": [
    "## Balanced Accuracy (Test at Peak Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea0023-0e48-4dda-92c8-2cbc1b06db6e",
   "metadata": {},
   "source": [
    "### Test @ Peak Validation **[MAIN RESULT]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfec9b2-e7d4-44f8-9096-5ed7d15236b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_peak_at_max_other('balanced_accuracy (test)', 'balanced_accuracy (validate)').sort_values(by='Mean').tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9494b-c626-41bf-8a18-29caa9a717a2",
   "metadata": {},
   "source": [
    "### Test @ Peak Test [Theoretical Potential]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da88fa4e-7163-433d-b01b-71e779b38e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_peak_at_max_other('balanced_accuracy (test)', 'balanced_accuracy (test)').sort_values(by='Mean').tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed3d404-e520-4ee5-9734-2ee0e00ad774",
   "metadata": {},
   "source": [
    "# Performance Across Trials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9383a84-3b77-4352-9e45-6b47fbd3151b",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eff682b-347b-4004-8adb-8470e7b8b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_performance_across_trials(df, metric, grouping, fpath):\n",
    "    # Plot the average and standard deviation\n",
    "    sns.lineplot(data=df, x='trial', y=metric, hue=grouping)\n",
    "\n",
    "    # Add details\n",
    "    plt.title(f'By {grouping.capitalize()} (Average)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    plt.savefig(fpath)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b116a5-ab3e-4387-b1f3-626b8e91b0c7",
   "metadata": {},
   "source": [
    "## Balanced Accuracy (Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248169f-4ee4-4115-b23c-e27441b58943",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = Path(\"figures/bacc_performance/\")\n",
    "for i in analysis_idx:\n",
    "    plot_average_performance_across_trials(performance_metric_df, 'balanced_accuracy (test)', i, output_dir/f'bacc_avg_by_{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474a6ad-9f6e-40ef-88be-ea416c9ca155",
   "metadata": {},
   "source": [
    "## Balanced Accuracy (Test) at Peak Balanced Accuracy (Validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43725ba8-1975-4b89-b429-c1e77ce6aea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric_at_peak_other_across_trials(df, metric, other, grouping, fpath):\n",
    "    # Reformat the data to be max by trial/replicate grouping\n",
    "    tmp_df = df.sort_values(other).groupby(['replicate', 'trial', grouping]).tail(1).reset_index()\n",
    "    \n",
    "    # Plot the average and standard deviation\n",
    "    sns.lineplot(data=tmp_df, x='trial', y=metric, hue=grouping)\n",
    "\n",
    "    # Add details\n",
    "    plt.title(f'By {grouping.capitalize()} (B.Acc Test @ Peak Validation)')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and show the plot\n",
    "    plt.savefig(fpath)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d9d92-9343-446f-bcbe-f39f1359a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in analysis_idx:\n",
    "    plot_metric_at_peak_other_across_trials(performance_metric_df, 'balanced_accuracy (test)', 'balanced_accuracy (validate)', i, output_dir/f'bacc_test_at_peak_validate_by_{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa491634-113a-4ce2-912a-758981068e16",
   "metadata": {},
   "source": [
    "## Balanced Accuracy (Test) Weighted by Balanced Accuracy (Validated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee2434b-76d4-46ff-a567-ea996f6ce7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_std(vals, weights):\n",
    "    mean_val = np.average(vals, weights=weights)\n",
    "    std_vals = np.average((vals-mean_val)**2, weights=weights)\n",
    "    return std_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714f118e-6bc9-4b31-9809-54e1abf1735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_weighted_by_other(df, metric, weight, grouping, fpath):\n",
    "    # Calculate the weighted metrics from the original dataset\n",
    "    df_groupedby = df.loc[:, [grouping, *study_idxs, metric, weight]].groupby([grouping, 'trial'])\n",
    "    mean_vals = df_groupedby.apply(lambda x: np.average(x[metric], weights=x[weight]), include_groups=False)\n",
    "    std_vals = df_groupedby.apply(lambda x: weighted_std(x[metric], x[weight]), include_groups=False)\n",
    "    \n",
    "    sub_df = pd.DataFrame()\n",
    "    sub_df['Mean'] = mean_vals\n",
    "    sub_df['STD'] = std_vals\n",
    "\n",
    "    # Plot each of them iteratively, w/ weighted mean and std\n",
    "    fig, ax = plt.subplots(1)\n",
    "    group_options = set(df[grouping])\n",
    "    for i, g in enumerate(group_options):\n",
    "        # Plot the main line\n",
    "        y = sub_df.reset_index().query(f\"{grouping} == '{g}'\")\n",
    "        y_mean = y.groupby('trial')['Mean'].mean()\n",
    "        ax.plot(y_mean, label=g)\n",
    "\n",
    "        # Plot the (weighted) standard deviation fills\n",
    "        y_std = y.groupby('trial')['STD'].mean()\n",
    "        ax.fill_between(np.arange(y_std.shape[0]), y_mean+y_std, y_mean-y_std, facecolor=f'C{i}', alpha=0.2)\n",
    "\n",
    "    # Add other plotted elements\n",
    "    plt.xlabel('Trial')\n",
    "    plt.ylabel('Weighted Average')\n",
    "    plt.legend(title=grouping)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e0313-55f1-4810-9ad9-bdbce9a6cc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in analysis_idx:\n",
    "    metric_weighted_by_other(performance_metric_df, 'balanced_accuracy (test)', 'balanced_accuracy (validate)', i, output_dir/f'bacc_weighted_avg_by_{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e99e8b-13ee-4ca6-8486-ac69f7d9ba8c",
   "metadata": {},
   "source": [
    "# Statistical Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb963b73-3a75-4c9c-a6ca-246f42f857cf",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabb0928-c546-4b1f-bc10-47c76357ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, permutations\n",
    "\n",
    "from scipy.stats import normaltest, ranksums, kruskal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74469ad1-a268-4bb7-9781-9662e5752be9",
   "metadata": {},
   "source": [
    "Target metric gathering function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e29e5-bfaa-406d-9228-4f01903673e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Absolute peak values by replicate, mean and std\n",
    "def get_best_per_replicate(target_value):\n",
    "    component_dfs = []\n",
    "    for k, df in df_map.items():\n",
    "        peak_df = df.sort_values(by=target_value).groupby('replicate').last()\n",
    "        peak_df = peak_df.loc[:, [*analysis_idx, 'trial', target_value]]\n",
    "        component_dfs.append(peak_df)\n",
    "    result_df = pd.concat(component_dfs).reset_index()\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c29494e-3290-4d5b-b382-cd2e1f2f495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values of one metric, sampled at the peak value of another, per-replicate mean and STD sampled\n",
    "def get_val_at_best_other_per_replicate(target, other):\n",
    "    component_dfs = []\n",
    "    for k, df in df_map.items():\n",
    "        peak_df = df.sort_values(by=other).groupby('replicate').last()\n",
    "        peak_df = peak_df.loc[:, [*analysis_idx, 'trial', target]]\n",
    "        component_dfs.append(peak_df)\n",
    "    result_df = pd.concat(component_dfs).reset_index()\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd440e39-1f2f-4b15-8010-623c78be99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_normality(df, query_key, target):\n",
    "    isnormal = {}\n",
    "    query_set = set(replicate_best_bacc_df[query_key])\n",
    "\n",
    "    for k in query_set:\n",
    "        x = df.query(f\"{query_key} == '{k}'\")[target]\n",
    "        isnormal[k] = [normaltest(x).pvalue]\n",
    "\n",
    "    # Save the results as a dataframe\n",
    "    return_df = pd.DataFrame.from_dict(isnormal).T\n",
    "    return_df.columns = ['p-value']\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c576c18-a23f-4aac-a534-1fccac6d7065",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_keys = {\n",
    "    'two-sided': '!=',\n",
    "    'greater':   '>',\n",
    "    'less':      '<'\n",
    "}\n",
    "\n",
    "def paired_rankedsum(df, query, target, alternative='two-sided'):\n",
    "    pvals = {}\n",
    "    query_set = set(df[query])\n",
    "\n",
    "    # Caclulate the native rankedsum p-value for each pair of datasets, testing whether the former's value is greater than the latters\n",
    "    for v1, v2 in permutations(query_set, 2):\n",
    "        x1 = df.query(f\"{query} == '{v1}'\")[target]\n",
    "        x2 = df.query(f\"{query} == '{v2}'\")[target]\n",
    "        p = ranksums(x1, x2, alternative=alternative).pvalue\n",
    "        pvals[f\"{v1} {alt_keys[alternative]} {v2}\"] = [p]\n",
    "\n",
    "    # Save the results as a dataframe\n",
    "    return_df = pd.DataFrame.from_dict(pvals).T\n",
    "    return_df.index.name = 'Comparison'\n",
    "    return_df.columns = ['p']\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4112b1d-99d1-4a9d-b939-0d4bfab9a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_kw(df, grouping, target):\n",
    "    query_set = set(df[grouping])\n",
    "    samples = [df.query(f\"{grouping} == '{q}'\")[target] for q in query_set]\n",
    "    return kruskal(*samples).pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a754f-899f-45c9-a9ea-d1a1e21b2fc0",
   "metadata": {},
   "source": [
    "## Testing Balanced Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5fb66-24ab-44d4-8ed3-a1faa838c327",
   "metadata": {},
   "source": [
    "### Testing @ Peak Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f258de9-0c58-423b-813d-cb4a4f4d5c1b",
   "metadata": {},
   "source": [
    "#### Raw Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3dda71-9fa8-47fe-a7e9-9cbac4ed718d",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'balanced_accuracy (test)'\n",
    "other = 'balanced_accuracy (validate)'\n",
    "replicate_test_at_peak_bacc_df = get_val_at_best_other_per_replicate(target, other)\n",
    "replicate_test_at_peak_bacc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d25719-926c-4fe5-8818-2fd245ba3107",
   "metadata": {},
   "source": [
    "#### Ranked-Sum Grouping Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8366d7ae-5bf1-4a4c-aa44-af4a2644f027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the p-values for whether one experimental permutation has greater average balanced accuracy performance than another\n",
    "sub_dfs = []\n",
    "for k in analysis_idx:\n",
    "    tmp_df = paired_rankedsum(replicate_test_at_peak_bacc_df, k, target, alternative='greater')\n",
    "    sub_dfs.append(tmp_df)\n",
    "\n",
    "sig_test_at_peak_valid_df = pd.concat(sub_dfs).sort_values('p')\n",
    "\n",
    "# Calculate the corrected p-value significance as well\n",
    "n_samples = sig_test_at_peak_valid_df.shape[0]\n",
    "sig_test_at_peak_valid_df['significance'] = ''\n",
    "for i, t in enumerate([0.05, 0.01, 0.001]):\n",
    "    sig_test_at_peak_valid_df.loc[sig_test_at_peak_valid_df['p']*n_samples < t, 'significance'] = '*'*(i+1)\n",
    "\n",
    "sig_test_at_peak_valid_df.reset_index().head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1e38bf-d76e-4598-a08c-04ad91479360",
   "metadata": {},
   "source": [
    "#### Kruskal-Wallace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379303ef-6892-47ba-b845-af452ddf7765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Kruskal-Wallace, confirm that there is a significant difference in the best-case performance for each analytical variation\n",
    "kw_pvals = {}\n",
    "for i in analysis_idx:\n",
    "    kw_pvals[i] = [evaluate_kw(replicate_test_at_peak_bacc_df, i, 'balanced_accuracy (test)')]\n",
    "kw_df = pd.DataFrame.from_dict(kw_pvals).T\n",
    "kw_df.columns = ['p']\n",
    "\n",
    "# Calculate the corrected p-value significance as well w/ Bonferroni correction\n",
    "kw_df['significance'] = ''\n",
    "n_samples = kw_df.shape[0]\n",
    "for i, t in enumerate([0.05, 0.01, 0.001]):\n",
    "    kw_df.loc[kw_df['p']*n_samples < t, 'significance'] = '*'*(i+1)\n",
    "\n",
    "kw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663f8e27-6b04-40cd-8153-3d12f85307c5",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cc2509-ff5c-4ec7-97e7-c9632dd3b45c",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74bd963-7fc0-49ae-b481-c667a37a7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_feature_imp(val):\n",
    "    # Strip leading and trailing brackets\n",
    "    val = val[1:-2]\n",
    "\n",
    "    # Create a dictionary from the remaining components\n",
    "    imp_dict = dict()\n",
    "    for v in val.split(', '):\n",
    "        vcomps = v.split(': ')\n",
    "        k = ': '.join(vcomps[:-1])\n",
    "        v = float(vcomps[-1])\n",
    "        imp_dict[k] = v\n",
    "        \n",
    "    return imp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d6d00a-69b7-44e1-870b-41de43d7ce8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_report(df: pd.DataFrame, weight_col, feature_col):\n",
    "    # Convert the dictionaries contained with the feature_col dicts into dataframes which can be stacked\n",
    "    raw_dfs = []\n",
    "    weighted_dfs = []\n",
    "    for r in df.iterrows():\n",
    "        rvals = r[1]\n",
    "        tmp_df = pd.DataFrame.from_dict({k: [v] for k, v in rvals[feature_col].items()})\n",
    "        weight = rvals[weight_col]\n",
    "        raw_dfs.append(tmp_df)\n",
    "        weighted_dfs.append(tmp_df * weight)\n",
    "\n",
    "    # Stack the dataframes\n",
    "    raw_feature_imps = pd.concat(raw_dfs).fillna(0)\n",
    "    weighted_feature_imps = pd.concat(weighted_dfs).fillna(0)\n",
    "\n",
    "    # Interpret the results into a clean report\n",
    "    feature_imp_report = {\n",
    "        \"Mean (Raw)\": raw_feature_imps.mean(),\n",
    "        \"STD (Raw)\": raw_feature_imps.std(),\n",
    "        \"Mean (Performance Weighted)\": weighted_feature_imps.mean(),\n",
    "        \"STD (Performance Weighted)\": weighted_feature_imps.std(),\n",
    "    }\n",
    "    result_df = pd.DataFrame.from_dict(feature_imp_report)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f5bee5-930a-448e-99be-dfcee856a75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_imp_report(df: pd.DataFrame, feature_col, weight_col) -> pd.DataFrame:\n",
    "    # Convert the dictionaries contained with the feature_col dicts into dataframes which can be stacked\n",
    "    raw_dfs = []\n",
    "    weighted_dfs = []\n",
    "    for r in df.iterrows():\n",
    "        rvals = r[1]\n",
    "        tmp_df = pd.DataFrame.from_dict({k: [v] for k, v in rvals[feature_col].items()})\n",
    "        raw_dfs.append(tmp_df)\n",
    "\n",
    "    # Stack the dataframes\n",
    "    raw_feature_imps = pd.concat(raw_dfs).fillna(0)\n",
    "\n",
    "    # Query the weights list a single time to avoid repeated querying expense\n",
    "    weights = df[weight_col]\n",
    "    \n",
    "    # For each feature, calculate our desired statistics\n",
    "    return_cols = ['Mean', 'STD', 'Weighted Mean', 'Weighted STD']\n",
    "    return_df_dict = {}\n",
    "    for c in raw_feature_imps.columns:\n",
    "        # Single query of the dataframe, as pandas can be slow w/ repeated queries\n",
    "        samples = raw_feature_imps[c]\n",
    "        # Raw Mean\n",
    "        c_mean = np.mean(samples)\n",
    "        # Raw STD\n",
    "        c_std = np.std(samples)\n",
    "        # Weighted mean\n",
    "        c_mean_weighted = np.average(samples, weights=weights)\n",
    "        # Weighted STD\n",
    "        c_std_weighted = weighted_std(samples, weights)\n",
    "        # Stack them into a list and store it in the dictionary\n",
    "        return_df_dict[c] = [c_mean, c_std, c_mean_weighted, c_std_weighted]\n",
    "\n",
    "    # Return the result as a dataframe\n",
    "    return pd.DataFrame.from_dict(return_df_dict, columns=return_cols, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6a5a1-b404-458d-8c48-96f958cea7c3",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1798fd-75d9-4364-8c9a-c549a655b1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate and stack the information relative to the value\n",
    "sub_dfs = []\n",
    "\n",
    "for df in df_map.values():\n",
    "    tmp_df = df.loc[:, [*study_idxs, *analysis_idx, 'balanced_accuracy (test)', 'importance_by_permutation (test)']]\n",
    "    sub_dfs.append(tmp_df)\n",
    "\n",
    "feature_imp_df = pd.concat(sub_dfs)\n",
    "\n",
    "# Isolate only the best trial from each replicate\n",
    "feature_imp_df = feature_imp_df.sort_values('balanced_accuracy (test)').groupby([*analysis_idx, 'replicate']).tail(1).set_index(analysis_idx)\n",
    "\n",
    "# Parse the feature importance list into a cleaner dictionary\n",
    "feature_imp_df['importance_by_permutation (test)'] = feature_imp_df['importance_by_permutation (test)'].apply(format_feature_imp)\n",
    "feature_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd1eaf4-20b7-4dc0-971f-eb8878f757a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate PCA-derived features from the rest\n",
    "pca_feature_imp_df = feature_imp_df.reset_index().loc[feature_imp_df.reset_index()['prep'].apply(lambda x: 'pca' in x), :].set_index([*analysis_idx])\n",
    "pca_feature_imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb2472b-9c5a-4984-ba62-c7b04eab4962",
   "metadata": {},
   "outputs": [],
   "source": [
    "nonpca_feature_imp_df = feature_imp_df.drop(pca_feature_imp_df.index)\n",
    "nonpca_feature_imp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174a8fbf-9283-45a1-9e22-fb7b33875ea6",
   "metadata": {},
   "source": [
    "## Un-transformed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aca5bb-c52e-4ed0-8430-22c1d7b80151",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feature_imp_df = nonpca_feature_imp_df.query(\"dataset == 'full'\")\n",
    "feature_imp_report(full_feature_imp_df, 'importance_by_permutation (test)', 'balanced_accuracy (test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a89023-df3e-480b-9358-d39635bafcf0",
   "metadata": {},
   "source": [
    "### Full dataset (all possible features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a715b7-8623-485f-9f75-b7cdf22cc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_feature_imp_df = nonpca_feature_imp_df.query(\"dataset == 'full'\")\n",
    "full_feature_report = feature_imp_report(full_feature_imp_df, 'importance_by_permutation (test)', 'balanced_accuracy (test)')\n",
    "full_feature_report.sort_values(\"Mean\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e51f0-642e-4da3-8def-88dd33110466",
   "metadata": {},
   "source": [
    "### Image-derived features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4954bda7-4083-4d28-97c3-9381d279ca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_feature_imp_df = nonpca_feature_imp_df.query(\"dataset == 'img_only'\")\n",
    "img_feature_report = feature_imp_report(img_feature_imp_df, 'importance_by_permutation (test)', 'balanced_accuracy (test)')\n",
    "img_feature_report.sort_values(\"Mean\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae32df16-f4b2-4db1-958f-c10a4184fb95",
   "metadata": {},
   "source": [
    "### Clinical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c18647-7cb0-40c0-82a8-a277a72110a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_feature_imp_df = nonpca_feature_imp_df.query(\"dataset == 'clinical'\")\n",
    "clin_feature_report = feature_imp_report(clin_feature_imp_df, 'importance_by_permutation (test)', 'balanced_accuracy (test)')\n",
    "clin_feature_report.sort_values(\"Mean\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0750b9-8536-4b82-83ee-b5c1807e97cb",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4c3c2-4897-430b-8157-2e023aee9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pca_imp_df = pca_feature_imp_df.query(\"dataset == 'full'\")\n",
    "full_pca_report = feature_imp_report(full_pca_imp_df, 'importance_by_permutation (test)', 'balanced_accuracy (test)')\n",
    "full_pca_report.sort_values(\"Mean\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f401f20-238d-4bd7-a6f1-ef6da5a7fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pca_imp_df = pca_feature_imp_df.query(\"dataset == 'img_only'\")\n",
    "img_pca_report = feature_imp_report(img_pca_imp_df, 'importance_by_permutation (test)', 'balanced_accuracy (test)')\n",
    "img_pca_report.sort_values(\"Mean\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4141c-3a43-44e9-a36a-c9f837191510",
   "metadata": {},
   "outputs": [],
   "source": [
    "clin_pca_imp_df = pca_feature_imp_df.query(\"dataset == 'clinical'\")\n",
    "clin_pca_report = feature_imp_report(clin_pca_imp_df, 'importance_by_permutation (test)', 'balanced_accuracy (test)')\n",
    "clin_pca_report.sort_values(\"Mean\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60268984-f2c9-483e-957a-fa5dd9dd5ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
