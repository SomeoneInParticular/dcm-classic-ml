from argparse import ArgumentParser
from pathlib import Path

import pandas as pd


def remove_dup_headers(df):
    """Removes duplicated headers generated by concatenating the results of multiple SCT calls"""
    df = df.loc[df.loc[:, "Timestamp"] != "Timestamp", :]
    return df


def parse_filenames(df):
    """ Parse the filenames to determine the orientation and contrast type of the image """
    # Get the root file names out of the paths in the dataset
    file_names = [f.split('/')[-1] for f in df['Filename']]

    # Split them into their notable components
    file_components = [f.split('_') for f in file_names]

    # Grab the patient ID from the components
    patient_ids = [f[0] for f in file_components]

    # Grab the orientation (acquisition type) from the components
    acqs = [f[1].split('-')[1] for f in file_components]

    # Get the contrast type of the image from the file components
    contrasts = [f[-2] for f in file_components]

    # Get the run number, if one exists, from the file components
    runs = [int(f[-3].split('-')[1]) if "run" in f[-3] else None for f in file_components]

    idx = pd.MultiIndex.from_tuples(
        zip(patient_ids, acqs, contrasts, runs, df['Slice (I->S)'].astype('int32')),
        names=['GRP', 'acq', 'weight', 'run', 'slice']
    )
    return idx


def main(input_file: Path, output_path: Path, label: str):
    # Load the data
    init_df = pd.read_csv(input_file)

    # De-duplicate headers
    init_df = remove_dup_headers(init_df)

    # Use the attributes derived from the file name as our new index
    df_idx = parse_filenames(init_df)
    init_df.index = df_idx

    # Drop some columns which aren't useful to the study
    to_drop = ['Timestamp', 'SCT Version', 'Filename', 'Slice (I->S)', 'VertLevel', 'DistancePMJ', 'SUM(length)']
    # Drop all variance-related metrics, as they cannot exist on per-slice measurements
    for c in init_df.columns:
        if "STD" in c:
            to_drop.append(c)
    init_df = init_df.drop(to_drop, axis=1)

    # Reformat the columns to strip the (redundant) MEAN label
    new_cols = [s.replace("MEAN(", '').replace(')', '') for s in init_df.columns]
    init_df.columns = new_cols

    # Convert everything to floats, as (due to the headers) everything is as string right now
    init_df = init_df.astype('float64')

    # Reset the index in preparation for statistics calculation
    init_df = init_df.reset_index()

    # Group the results in preparation for statistics calculation
    df_groups = init_df.groupby(['GRP', 'acq', 'weight', 'run'], dropna=False)

    # Map of DF by statistic to make iteratively building up the dataframe easier later
    stat_df_map = {
        "Min.": df_groups.min(),
        "Max.": df_groups.max(),
        "Mean": df_groups.mean(),
        "Std.": df_groups.std()
    }

    # Rename the columns of each dataframe in the map to include their respective statistic
    for k, v in stat_df_map.items():
        v.columns = [f"{c} ({k})" for c in v.columns]

    # Append all the features together
    result_df = pd.concat(stat_df_map.values(), axis=1)

    # Sort the index alphabetically for easier review
    result_df = result_df.reindex(sorted(result_df.columns), axis=1)

    # Keep only the last run for each sample with multiple runs
    result_df = result_df.reset_index().sort_values('run').groupby(['GRP', 'acq', 'weight']).last()

    # Split the dataset by contrast and orientation, saving the result only if 50 or more samples exist
    img_dfs = dict()
    for idx, df in result_df.reset_index().groupby(['acq', 'weight']):
        # If there are less than 50 samples for this grouping, skip over it with a message
        n = df.shape[0]
        if n < 50:
            print(f"Orientation '{idx[0]}' and weight '{idx[1]}' had only {n} samples, and was skipped.")
            continue
        # Otherwise, track the results by a label (being the combination of orientation and weight)
        img_dfs['_'.join(idx)] = df

    # If the output path doesn't exist, make it
    if not output_path.exists():
        output_path.mkdir(parents=True)

    # Save the datasets which were found prior iteratively
    for k, df in img_dfs.items():
        output_file = output_path / f"{label}_{k}.tsv"
        df.set_index('GRP').drop(columns=['acq', 'weight', 'run']).to_csv(output_file, sep='\t')


if __name__ == '__main__':
    # Parse command line arguments if this script is run directly
    parser = ArgumentParser()

    parser.add_argument(
        '-i', '--input_file', required=True, type=Path,
        help="The initial dataset file to use, generated by concatenating a set of files output from "
             "`sct_process_segmentation`. As such, this is assumed to be in '.csv' format."
    )
    parser.add_argument(
        '-o', '--output_path', required=True, type=Path,
        help="The path to place the cleaned results. Do NOT include a filename, as this script can produce "
             "multiple files"
    )
    parser.add_argument(
        '--label', default='img', type=str,
        help="The 'base' label that the output files will have, appended before the orientation and contrast."
    )

    kwargs = parser.parse_args().__dict__
    main(**kwargs)